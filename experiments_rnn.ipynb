{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from collections import defaultdict\n",
    "from dfa import Dfa\n",
    "from model import SequenceModel, Ngram, GaussianProbe\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch import nn, optim\n",
    "import torch\n",
    "import json\n",
    "\n",
    "N_STATES = 8\n",
    "N_SYMBOLS = 128\n",
    "N_ADJACENCIES = 4\n",
    "\n",
    "#N_SYMBOLS = 8\n",
    "#N_ADJACENCIES = 4\n",
    "\n",
    "N_ITERS = 2000\n",
    "\n",
    "def train_model(dfa, rand, train, **train_params):\n",
    "    return SequenceModel.train_model(dfa, rand, 256, 256, max_iters=(N_ITERS if train else 0), model_type=\"transformer\", **train_params)\n",
    "\n",
    "def train_ngram_model(dfa, rand, max_order):\n",
    "    return Ngram.train_model(dfa, rand, max_order, n_samples=128 * 1000 * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_interpolator(contexts, true_label, pred_labels, add=True):\n",
    "    class Model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.params = nn.ParameterList([nn.Parameter(torch.tensor(0).float().cuda()) for _ in pred_labels])\n",
    "            assert len(pred_labels) == 2\n",
    "            #self.param = nn.Parameter(torch.tensor(0).float().cuda())\n",
    "            self.loss = nn.MSELoss(reduction=\"none\")\n",
    "        \n",
    "        def forward(self, true_dist, pred_dists):\n",
    "            if add:\n",
    "                param_sum = sum([torch.exp(p) for p in self.params])\n",
    "                params_norm = [torch.exp(p) / param_sum for p in self.params]\n",
    "                #param = torch.sigmoid(self.param)\n",
    "                #oparam = 1 - param\n",
    "            else:\n",
    "                #true_dist = torch.log(true_dist + 1e-6)\n",
    "                pred_dists = [torch.log(dist + 1e-6) for dist in pred_dists]\n",
    "                params_norm = [torch.exp(p) for p in self.params]\n",
    "                #param = self.param\n",
    "                #oparam = -param\n",
    "            pred = [param * dist for param, dist in zip(params_norm, pred_dists)]\n",
    "            pred = sum(pred)\n",
    "            #if not add:\n",
    "            #    print(param, pred_dists[0][:10])\n",
    "            #    print(oparam, pred_dists[1][:10])\n",
    "            #pred = param * pred_dists[0] + oparam * pred_dists[1]\n",
    "            if not add:\n",
    "                #print(pred.shape)\n",
    "                #assert False\n",
    "                pred = pred.softmax(dim=1)\n",
    "                assert (pred == pred).all(), (params_norm, pred_dists[0][0, :10], pred_dists[1][0, :10], pred[0, :10])\n",
    "            #print(pred_dists)\n",
    "            #print(pred)\n",
    "            #print(true_dist)\n",
    "            loss = self.loss(pred, true_dist).sum(dim=1).mean(dim=0)\n",
    "            err = torch.abs(pred - true_dist).sum(dim=1).mean(dim=0) / 2\n",
    "            assert err <= 1, (pred[0, :10], true_dist[0, :10])\n",
    "            #if not add:\n",
    "            #    print(pred[0][:10])\n",
    "            #    print(loss, err)\n",
    "            return loss, err\n",
    "        \n",
    "    model = Model()\n",
    "    opt = optim.Adam(model.parameters(), lr=0.01)\n",
    "    true_data = torch.tensor([contexts[k][true_label] for k in contexts]).float().cuda()\n",
    "    pred_data = [torch.tensor([contexts[k][pred_label] for k in contexts]).float().cuda() for pred_label in pred_labels]\n",
    "    for i in range(200):\n",
    "        loss, err = model(true_data, pred_data)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "    #if not add:\n",
    "    #    assert False\n",
    "    \n",
    "    return err.item(), [p.item() for p in model.params]\n",
    "\n",
    "def run_experiment_1(dfa, data_rand, hypotheses, **train_params):\n",
    "    rnn = train_model(dfa, data_rand, train=True, **train_params)\n",
    "    rnn_copy = train_model(dfa, data_rand, train=True, **train_params)\n",
    "    ngram = None\n",
    "    if any(\"gram\" in name for name, _ in hypotheses):\n",
    "        ngram = train_ngram_model(dfa, data_rand, max_order=2)\n",
    "    \n",
    "    ghost_symbols = []\n",
    "    real_symbols = []\n",
    "    while len(ghost_symbols) < 100:\n",
    "        prefix = dfa.sample(data_rand)\n",
    "        if len(prefix) == 0:\n",
    "            continue\n",
    "        prefix_len = data_rand.randint(len(prefix))\n",
    "        prefix_symbols = prefix[:prefix_len]\n",
    "        ann = dfa.annotate(prefix_symbols)\n",
    "        prefix_state = ann[-1]\n",
    "        edges = dfa.edges[prefix_state]\n",
    "        \n",
    "        forbidden_symbols = [sym for sym, dst in edges] + dfa.unused_symbols\n",
    "        ghost_choices = [i for i in range(dfa.n_symbols) if i not in forbidden_symbols]\n",
    "        if len(ghost_choices) == 0:\n",
    "            continue\n",
    "        ghost_symbol = data_rand.choice(ghost_choices)\n",
    "        ghost_symbols.append(prefix_symbols + (ghost_symbol,))\n",
    "        \n",
    "        real_choices = [sym for sym, dst in edges if sym is not None]\n",
    "        real_symbol = data_rand.choice(real_choices)\n",
    "        real_symbols.append(prefix_symbols + (real_symbol,))\n",
    "    \n",
    "    results = []\n",
    "    ghost_contexts = defaultdict(dict)\n",
    "    real_contexts = defaultdict(dict)\n",
    "    for hypothesis_name, hypothesis in hypotheses:\n",
    "        total_interp_dist = 0\n",
    "        total_extrap_dist = 0\n",
    "        count = 0\n",
    "        for i in range(len(ghost_symbols)):\n",
    "            \n",
    "            if False:\n",
    "                rs = real_symbols[i]\n",
    "                true_states = dfa.annotate(rs[:-1], allow_incomplete=True)\n",
    "                if rs in real_contexts:\n",
    "                    true_preds = real_contexts[rs][\"TRUE\"]\n",
    "                    copy_preds = real_contexts[rs][\"COPY\"]\n",
    "                    lookahead_preds = real_contexts[rs][\"LOOKAHEAD\"]\n",
    "                else:\n",
    "                    true_preds, = rnn.predict_one([rs])\n",
    "                    copy_preds, = rnn_copy.predict_one([rs])\n",
    "                    #lookahead_preds = rnn.predict_two([rs], 2 * N_ADJACENCIES, data_rand)\n",
    "                    lookahead_preds = None\n",
    "                    real_contexts[rs][\"TRUE\"] = true_preds\n",
    "                    real_contexts[rs][\"COPY\"] = copy_preds\n",
    "                    real_contexts[rs][\"LOOKAHEAD\"] = lookahead_preds\n",
    "                real_preds = hypothesis(\n",
    "                    dfa, rs, true_states,\n",
    "                    model_preds=true_preds, copy_preds=copy_preds, ngram_model=ngram,\n",
    "                    lookahead_preds=lookahead_preds,\n",
    "                    **train_params\n",
    "                )\n",
    "                real_contexts[rs][hypothesis_name.strip()] = real_preds\n",
    "                total_interp_dist += np.abs(true_preds - real_preds).sum() / 2\n",
    "            \n",
    "            # -----\n",
    "            \n",
    "            gs = ghost_symbols[i]\n",
    "            true_states = dfa.annotate(gs[:-1], allow_incomplete=True)\n",
    "            if gs in ghost_contexts:\n",
    "                true_preds = ghost_contexts[gs][\"TRUE\"]\n",
    "                copy_preds = ghost_contexts[gs][\"COPY\"]\n",
    "                lookahead_preds = ghost_contexts[gs][\"LOOKAHEAD\"]\n",
    "            else:\n",
    "                true_preds, = rnn.predict_one([gs])\n",
    "                copy_preds, = rnn_copy.predict_one([gs])\n",
    "                #lookahead_preds = rnn.predict_two([gs], 2 * N_ADJACENCIES, data_rand)\n",
    "                lookahead_preds = None\n",
    "                ghost_contexts[gs][\"TRUE\"] = true_preds\n",
    "                ghost_contexts[gs][\"COPY\"] = copy_preds\n",
    "                ghost_contexts[gs][\"LOOKAHEAD\"] = lookahead_preds\n",
    "\n",
    "            ghost_preds = hypothesis(\n",
    "                dfa, gs, true_states, \n",
    "                model_preds=true_preds, copy_preds=copy_preds, ngram_model=ngram,\n",
    "                lookahead_preds=lookahead_preds,\n",
    "                **train_params\n",
    "            )\n",
    "            ghost_contexts[gs][hypothesis_name.strip()] = ghost_preds\n",
    "            total_extrap_dist += np.abs(true_preds - ghost_preds).sum() / 2\n",
    "            \n",
    "            # -----\n",
    "            \n",
    "            count += 1\n",
    "            \n",
    "        results.append({\n",
    "            \"n_states\": int(dfa.n_states),\n",
    "            \"n_symbols\": int(dfa.n_symbols),\n",
    "            \"n_neighbors\": int(dfa.n_neighbors),\n",
    "            \"hypothesis\": hypothesis_name.strip(),\n",
    "            \"interp_acc\": float(total_interp_dist / count),\n",
    "            \"extrap_acc\": float(total_extrap_dist / count),\n",
    "            **{k: float(v) for k, v in train_params.items()}\n",
    "        })\n",
    "        \n",
    "    #interp_interpolated_acc, interp_interpolation_weights = compute_interpolator(real_contexts, \"TRUE\", [\"ngram(2)\", \"mean_dist_state\"])\n",
    "    extrap_interpolated_acc_add, extrap_interpolation_weights_add = compute_interpolator(ghost_contexts, \"TRUE\", [\"ngram(2)\", \"mean_dist_state\"], add=True)\n",
    "    extrap_interpolated_acc_mul, extrap_interpolation_weights_mul = compute_interpolator(ghost_contexts, \"TRUE\", [\"ngram(2)\", \"mean_dist_state\"], add=False)\n",
    "    results.append({\n",
    "        \"n_states\": int(dfa.n_states),\n",
    "        \"n_symbols\": int(dfa.n_symbols),\n",
    "        \"n_neighbors\": int(dfa.n_neighbors),\n",
    "        \"hypothesis\": \"interp_add\",\n",
    "        #\"interp_acc\": interp_interpolated_acc,\n",
    "        \"extrap_acc\": float(extrap_interpolated_acc_add),\n",
    "        #\"interp_weight_ngram\": interp_interpolation_weights[0],\n",
    "        #\"interp_weight_out\": interp_interpolation_weights[1],\n",
    "        \"extrap_weight_ngram\": float(extrap_interpolation_weights_add[0]),\n",
    "        \"extrap_weight_out\": float(extrap_interpolation_weights_add[1]),\n",
    "        **{k: float(v) for k, v in train_params.items()}\n",
    "    })\n",
    "    \n",
    "    results.append({\n",
    "        \"n_states\": int(dfa.n_states),\n",
    "        \"n_symbols\": int(dfa.n_symbols),\n",
    "        \"n_neighbors\": int(dfa.n_neighbors),\n",
    "        \"hypothesis\": \"interp_mul\",\n",
    "        #\"interp_acc\": interp_interpolated_acc,\n",
    "        \"extrap_acc\": float(extrap_interpolated_acc_mul),\n",
    "        #\"interp_weight_ngram\": interp_interpolation_weights[0],\n",
    "        #\"interp_weight_out\": interp_interpolation_weights[1],\n",
    "        \"extrap_weight_ngram\": float(extrap_interpolation_weights_mul[0]),\n",
    "        \"extrap_weight_out\": float(extrap_interpolation_weights_mul[1]),\n",
    "        **{k: float(v) for k, v in train_params.items()}\n",
    "    })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyp_rand = np.random.RandomState(0)\n",
    "\n",
    "def random_out_state_hyp(dfa, symbols, true_states, **kwargs):\n",
    "    state_edges = dfa.edges[true_states[-1]]\n",
    "    state_edges = [edge for edge in state_edges if edge != (None, None)]\n",
    "    reachable = sorted(set(state for _, state in state_edges))\n",
    "    predicted_state = hyp_rand.choice(reachable)\n",
    "    return dfa.predict_one(predicted_state)\n",
    "    \n",
    "def random_in_state_hyp(dfa, symbols, true_states, **kwargs):\n",
    "    reachable = [nstate for state_edges in dfa.edges for symbol, nstate in state_edges if symbol == symbols[-1]]\n",
    "    reachable = sorted(set(reachable))\n",
    "    predicted_state = hyp_rand.choice(reachable)\n",
    "    return dfa.predict_one(predicted_state)\n",
    "\n",
    "def mean_out_state_hyp(dfa, symbols, true_states, **kwargs):\n",
    "    state_edges = dfa.edges[true_states[-1]]\n",
    "    state_edges = [edge for edge in state_edges if edge != (None, None)]\n",
    "    reachable = sorted(set(state for _, state in state_edges))\n",
    "    dists = [dfa.predict_one(state) for state in reachable]\n",
    "    #print(reachable)\n",
    "    return np.mean(dists, axis=0)\n",
    "\n",
    "def mean_dist_state_hyp(dfa, symbols, true_states, **kwargs):\n",
    "    dists = 0\n",
    "    weights = 0\n",
    "    for state in range(dfa.n_states):\n",
    "        weight = np.exp(-dfa.distances[true_states[-1]][state])\n",
    "        #print(weight)\n",
    "        dists += (weight * dfa.predict_one(state))\n",
    "        weights += weight\n",
    "    dist = dists / weights\n",
    "    return dist\n",
    "\n",
    "def mean_in_state_hyp(dfa, symbols, true_states, **kwargs):\n",
    "    reachable = [nstate for state_edges in dfa.edges for symbol, nstate in state_edges if symbol == symbols[-1]]\n",
    "    reachable = sorted(set(reachable))\n",
    "    dists = [dfa.predict_one(state) for state in reachable]\n",
    "    #print(reachable)\n",
    "    return np.mean(dists, axis=0)\n",
    "\n",
    "def oracle_out_state_hyp(dfa, symbols, true_states, model_preds, **kwargs):\n",
    "    state_edges = dfa.edges[true_states[-1]]\n",
    "    state_edges = [edge for edge in state_edges if edge != (None, None)]\n",
    "    reachable = sorted(set(state for _, state in state_edges))\n",
    "    dists = [dfa.predict_one(state) for state in reachable]\n",
    "    return min(dists, key=lambda x: np.abs(x - model_preds).sum())\n",
    "\n",
    "def oracle_in_state_hyp(dfa, symbols, true_states, model_preds, **kwargs):\n",
    "    reachable = [nstate for state_edges in dfa.edges for symbol, nstate in state_edges if symbol == symbols[-1]]\n",
    "    reachable = sorted(set(reachable))\n",
    "    dists = [dfa.predict_one(state) for state in reachable]\n",
    "    return min(dists, key=lambda x: np.abs(x - model_preds).sum())\n",
    "\n",
    "def copycat_hyp(dfa, symbols, true_states, copy_preds, **kwargs):\n",
    "    return copy_preds\n",
    "\n",
    "def lookahead_hyp(dfa, symbols, true_states, lookahead_preds, **kwargs):\n",
    "    return sum(lookahead_preds) / len(lookahead_preds)\n",
    "\n",
    "def skip_hyp(dfa, symbols, true_states, **kwargs):\n",
    "    return dfa.predict_one(true_states[-1])\n",
    "\n",
    "def gt_hyp(dfa, symbols, true_states, **kwargs):\n",
    "    try:\n",
    "        state = dfa.annotate(symbols)[-1]\n",
    "        #print(\"gt state\", state)\n",
    "        return dfa.predict_one(state)\n",
    "    except:\n",
    "        return unif_hyp(dfa, symbols, true_states, **kwargs)\n",
    "\n",
    "def ngram_hyp(order):\n",
    "    assert order > 0\n",
    "    def fn(dfa, symbols, true_states, ngram_model, **kwargs):\n",
    "        if order == 1:\n",
    "            dist = ngram_model.predict_symbol(())\n",
    "        else:\n",
    "            dist = ngram_model.predict_symbol(symbols[-order+1:])\n",
    "        out = np.zeros(N_SYMBOLS+1)\n",
    "        for i in range(N_SYMBOLS):\n",
    "            if i in dist:\n",
    "                out[i] = dist[i]\n",
    "        \n",
    "        if None in dist:\n",
    "            out[-1] = dist[None]\n",
    "            \n",
    "       # print(\"orig\", out.shape)\n",
    "        \n",
    "        #display(dfa.render())\n",
    "        #print([chr(ord(\"a\") + s) for s in symbols])\n",
    "        #print(model_preds)\n",
    "        #print(out)\n",
    "        #print(np.abs(model_preds - out))\n",
    "        #assert False\n",
    "        return out\n",
    "    return fn\n",
    "\n",
    "def ngram_state_hyp(order):\n",
    "    assert order > 0\n",
    "    def fn(dfa, symbols, true_states, ngram_model, **kwargs):\n",
    "        states = ngram_model.predict_state(symbols[-order:])\n",
    "        #print(\"inferred states\", states)\n",
    "        out = np.zeros(N_SYMBOLS+1)\n",
    "        for state in states:\n",
    "            out += dfa.predict_one(state) * states[state]\n",
    "        assert abs(out.sum() - 1) < 1e-4\n",
    "        \n",
    "        direct = ngram_hyp(order)(dfa, symbols, true_states, ngram_model)\n",
    "        #print(out)\n",
    "        #print(direct)\n",
    "        #print([chr(ord(\"a\") + s) for s in symbols])\n",
    "        \n",
    "        #print(out)\n",
    "        #print(gt_hyp(dfa, symbols, true_states, **kwargs))\n",
    "        #print()\n",
    "        #print(out.shape)\n",
    "        return out\n",
    "    return fn\n",
    "\n",
    "def mean_both_state_hyp(dfa, symbols, true_states, ngram_model, **kwargs):\n",
    "    #return (mean_in_state_hyp(dfa, symbols, true_states) + mean_out_state_hyp(dfa, symbols, true_states)) / 2\n",
    "    return (\n",
    "        ngram_state_hyp(2)(dfa, symbols, true_states, ngram_model)\n",
    "        + mean_out_state_hyp(dfa, symbols, true_states)\n",
    "    ) / 2\n",
    "\n",
    "def unif_hyp(dfa, symbols, true_states, **kwargs):\n",
    "    return np.ones(dfa.n_symbols + 1, dtype=np.float32) / (dfa.n_symbols + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [09:47<00:00, 195.99s/it]\n",
      "100%|██████████| 3/3 [11:52<00:00, 237.35s/it]\n",
      " 33%|███▎      | 1/3 [03:57<07:55, 237.55s/it]"
     ]
    }
   ],
   "source": [
    "results1 = []\n",
    "\n",
    "configs = [\n",
    "    #{\"symbol_swap\": [0, 1/2]},\n",
    "    {\"symbol_swap\": [0, 1/4, 1/2, 3/4]},\n",
    "    {\"symbol_mask\": [0, 1/4, 1/2, 3/4]},\n",
    "    {\"symbol_dropout\": [0, 1/4, 1/2, 3/4]},\n",
    "    {\"state_reset\": [0, 1/4, 1/2, 3/4]},\n",
    "    {\"state_skip\": [0, 1/4, 1/2, 3/4]},\n",
    "    {\"state_noise\": [0, 1/2, 1, 2]},\n",
    "    {\"state_dropout\": [0, 1/4, 1/2, 3/4]}\n",
    "]\n",
    "\n",
    "for config in configs:\n",
    "    assert len(config) == 1\n",
    "    noise_type, = config.keys()\n",
    "    noise_values = config[noise_type]\n",
    "    result_group = []\n",
    "    for noise_value in noise_values:\n",
    "        dfa_rand = np.random.RandomState(0)\n",
    "        for i in tqdm(range(3)):\n",
    "            dfa = Dfa.generate_balanced(dfa_rand, N_STATES, N_SYMBOLS, N_ADJACENCIES)\n",
    "            hypotheses = [\n",
    "                (\"copycat          \", copycat_hyp),\n",
    "                (\"gt               \", gt_hyp),\n",
    "                #(\"ngram(2)x         \", ngram_hyp(4)),\n",
    "                #(\"ngram(4)   \", ngram_state_hyp(4)),\n",
    "                (\"ngram(2)   \", ngram_hyp(2)),\n",
    "                (\"ngram_state(2)\", ngram_state_hyp(2)),\n",
    "                (\"ngram(1)   \", ngram_hyp(1)),\n",
    "                (\"skip\", skip_hyp),\n",
    "                #(\"lookahead        \", lookahead_hyp),\n",
    "                (\"mean_dist_state  \", mean_dist_state_hyp),\n",
    "            ]\n",
    "            #display(dfa.render())\n",
    "            data_rand = np.random.RandomState(0)\n",
    "            result_group += run_experiment_1(\n",
    "                dfa,\n",
    "                data_rand,\n",
    "                hypotheses,\n",
    "                **{noise_type: noise_value}\n",
    "            )\n",
    "            #assert False\n",
    "\n",
    "    results1.append(result_group)\n",
    "    result_group = pd.DataFrame(result_group)\n",
    "\n",
    "    #sns.relplot(data=result_group, hue=\"hypothesis\", x=noise_type, y=\"interp_acc\", aspect=1, kind=\"line\")\n",
    "    #plt.title(noise_type)\n",
    "    #plt.ylim(0, 0.5)\n",
    "    #plt.show()\n",
    "    \n",
    "    sns.relplot(data=result_group, hue=\"hypothesis\", x=noise_type, y=\"extrap_acc\", aspect=1, kind=\"line\")\n",
    "    plt.title(noise_type)\n",
    "    plt.ylim(0, 0.5)\n",
    "    plt.show()\n",
    "    \n",
    "    #sns.relplot(\n",
    "    #    data=result_group.melt(id_vars=[noise_type], value_vars=[\"interp_weight_ngram\", \"interp_weight_out\"]),\n",
    "    #    x=noise_type, y=\"value\", hue=\"variable\", aspect=1, kind=\"line\"\n",
    "    #)\n",
    "    #plt.show()\n",
    "    \n",
    "    sns.relplot(\n",
    "        data=result_group.melt(id_vars=[noise_type], value_vars=[\"extrap_weight_ngram\", \"extrap_weight_out\"]),\n",
    "        x=noise_type, y=\"value\", hue=\"variable\", aspect=1, kind=\"line\"\n",
    "        #data=result_group, x=noise_type, y=\"extrap_weight\", hue=\"hypothesis\", aspect=1, kind=\"line\"\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "with open(\"results1.json\", \"w\") as writer:\n",
    "    json.dump(results1, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    sns.relplot(\n",
    "        data=result_group.melt(id_vars=[noise_type], value_vars=[\"extrap_weight_ngram\", \"extrap_weight_out\"]),\n",
    "        x=noise_type, y=\"value\", hue=\"variable\", aspect=1, kind=\"line\"\n",
    "        #data=result_group, x=noise_type, y=\"extrap_weight\", hue=\"hypothesis\", aspect=1, kind=\"line\"\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(results1[0][0])\n",
    "#for k, v in results1[0][0].items():\n",
    "#    print(k, type(v))\n",
    "#with open(\"results1.json\", \"w\") as writer:\n",
    "#    json.dump(results1[0][0], writer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
